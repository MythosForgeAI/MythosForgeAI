<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MythosForgeAI - Prototype</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom scrollbar for chat (optional) */
        #chat-output::-webkit-scrollbar {
            width: 8px;
        }
        #chat-output::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 10px;
        }
        #chat-output::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 10px;
        }
        #chat-output::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
        .chat-bubble {
            max-width: 75%;
            padding: 10px 15px;
            border-radius: 20px;
            margin-bottom: 10px;
            word-wrap: break-word;
        }
        .user-bubble {
            background-color: #DCF8C6; /* Light green */
            align-self: flex-end;
            border-bottom-right-radius: 5px;
        }
        .ai-bubble {
            background-color: #E5E7EB; /* Light gray */
            align-self: flex-start;
            border-bottom-left-radius: 5px;
        }
        .message-container {
            display: flex;
            flex-direction: column;
        }
    </style>
</head>
<body class="bg-gray-100 flex flex-col items-center justify-center min-h-screen font-sans p-4">

    <div class="w-full max-w-2xl bg-white shadow-xl rounded-lg flex flex-col h-[80vh]">
        <header class="bg-indigo-600 text-white p-4 rounded-t-lg">
            <h1 class="text-2xl font-semibold text-center">MythosForgeAI - Chat Prototype</h1>
        </header>

        <div id="chat-output" class="flex-1 p-6 space-y-4 overflow-y-auto message-container">
            <div class="chat-bubble ai-bubble">
                Hello Kyle! I'm your MythosForgeAI prototype. Send me a message to test the Netlify proxy!
            </div>
        </div>

        <div class="bg-gray-50 p-4 border-t border-gray-200 rounded-b-lg">
            <div class="flex items-center space-x-3">
                <input type="text" id="message-input" class="flex-1 p-3 border border-gray-300 rounded-lg focus:ring-2 focus:ring-indigo-500 focus:border-transparent outline-none" placeholder="Type your message...">
                <button id="send-button" class="bg-indigo-600 hover:bg-indigo-700 text-white font-semibold py-3 px-6 rounded-lg transition duration-150 ease-in-out">
                    Send
                </button>
            </div>
        </div>
    </div>

    <script>
        const chatOutput = document.getElementById('chat-output');
        const messageInput = document.getElementById('message-input');
        const sendButton = document.getElementById('send-button');

        // Stores the conversation history for context
        let conversationHistory = [
            // Initialize with a system message or AI's first message if needed by your LLM/proxy
            // For Gemini, the history usually starts with a user message or a model response.
            // If your proxy adds a system prompt, you might not need one here.
            // Example: { role: "system", content: "You are a helpful AI."} // OpenAI style
            // Example: { role: "user", parts: [{text: "Initial context for model (invisible to user)"}]}, 
            //          { role: "model", parts: [{text: "Okay, I'm ready."}]} // Gemini style
            { role: "model", parts: [{ text: "Hello Kyle! I'm your MythosForgeAI prototype. Send me a message to test the Netlify proxy!" }] }
        ];

        /**
         * Displays a message in the chat output area.
         * @param {string} message - The message text.
         * @param {string} sender - "User" or "AI".
         */
        function displayMessage(message, sender) {
            const messageElement = document.createElement('div');
            messageElement.classList.add('chat-bubble');
            if (sender === 'User') {
                messageElement.classList.add('user-bubble');
                messageElement.textContent = `You: ${message}`;
            } else {
                messageElement.classList.add('ai-bubble');
                messageElement.textContent = `AI: ${message}`;
            }
            chatOutput.appendChild(messageElement);
            chatOutput.scrollTop = chatOutput.scrollHeight; // Auto-scroll to bottom
        }

        /**
         * Gets a response from the AI via your Netlify backend proxy.
         * @param {string} userMessage - The user's message.
         */
        async function getAIResponse(userMessage) {
            // Add user message to internal history for context before sending
            conversationHistory.push({ role: "user", parts: [{ text: userMessage }] });

            const yourBackendProxyUrl = "https://mythosforgeai.com/.netlify/functions/llm-proxy"; 

            console.log("Sending to Netlify proxy:", userMessage);
            console.log("Sending conversation history:", JSON.stringify(conversationHistory, null, 2));

            let aiResponseText = "Error connecting to AI. Please try again."; // Default error

            try {
                // The payload structure should match what your Netlify function (llm-proxy.js) expects
                const payload = {
                    history: conversationHistory, 
                    // message: userMessage // Your llm-proxy.js expects 'history' and 'message'
                                         // However, the userMessage is already the last item in 'history'
                                         // Let's ensure the proxy handles this structure.
                                         // Sending just the history which now includes the latest user message.
                                         // The proxy can then take `history` and pass it as `contents` to Gemini.
                };

                const response = await fetch(yourBackendProxyUrl, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify(payload) // Send the payload your proxy expects
                });

                if (!response.ok) {
                    const errorBody = await response.text(); // Get more details from error
                    console.error(`Proxy Error: ${response.status}`, errorBody);
                    aiResponseText = `Error from AI service: ${response.status}. Check console for details.`;
                    // Display the error message or a user-friendly version
                } else {
                    const result = await response.json(); // Assuming your proxy returns JSON: { "aiMessage": "..." }
                    if (result.aiMessage) {
                        aiResponseText = result.aiMessage;
                    } else if (result.error) { // Handle errors returned by your proxy
                        console.error("Error from proxy function:", result.error);
                        aiResponseText = `Proxy function error: ${result.error}`;
                    } else {
                        console.error("Unexpected response structure from proxy:", result);
                        aiResponseText = "Received an unexpected response from the AI service.";
                    }
                }

            } catch (error) {
                console.error("Fetch error calling your backend proxy:", error);
                aiResponseText = "Network error or proxy is unreachable. Please try again.";
            }
            
            // Add AI response to internal history
            if (aiResponseText) {
                conversationHistory.push({ role: "model", parts: [{ text: aiResponseText }] });
            }
            
            // Limit history size (basic trimming)
            const maxHistoryLength = 20; // e.g., keep last 10 turns (user + AI)
            if (conversationHistory.length > maxHistoryLength) {
                conversationHistory = conversationHistory.slice(conversationHistory.length - maxHistoryLength);
            }

            if (aiResponseText) {
                 displayMessage(aiResponseText, 'AI');
            }
        }

        /**
         * Handles sending the user's message.
         */
        function handleSendMessage() {
            const message = messageInput.value.trim();
            if (message) {
                displayMessage(message, 'User'); // Display user message immediately
                messageInput.value = ''; // Clear input field
                getAIResponse(message); // Then get AI response
            }
        }

        sendButton.addEventListener('click', handleSendMessage);
        messageInput.addEventListener('keypress', function(event) {
            if (event.key === 'Enter') {
                handleSendMessage();
            }
        });

    </script>
</body>
</html>
```

**Key changes in this version's JavaScript:**

1.  **`yourBackendProxyUrl`:** This is now set to `https://mythosforgeai.com/.netlify/functions/llm-proxy`.
2.  **Real `fetch` Call:** The mock AI response section is removed, and the `fetch` call to your Netlify function is active.
3.  **Payload to Proxy:** The `payload` sent to your Netlify function now sends the `conversationHistory`. Your `llm-proxy.js` function should be expecting a body like `{ "history": [...] }` and then it will use that `history` as the `contents` for the Gemini API call.
4.  **Error Handling:** Basic error handling for the `fetch` call and if the response from the proxy isn't `ok` or doesn't have the expected `aiMessage` property.

**Before you deploy this updated HTML to GitHub:**

* **Ensure your `llm-proxy.js` on Netlify is correctly set up to:**
    * Expect a JSON body with a `history` property (which is an array of message objects).
    * Use this `history` array as the `contents` field when calling the Gemini API.
    * Return a JSON response in the format `{ "aiMessage": "the AI's actual response here" }` or `{ "error": "some error message" }`.

Once you push this updated HTML to GitHub, Netlify should redeploy your site, and it will start using your Netlify function to communicate with the LLM API! Good luck with the testi
